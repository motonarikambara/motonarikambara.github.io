<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Motonari Kambara</title><link>https://motonarikambara.github.io/</link><description>Recent content on Motonari Kambara</description><generator>Hugo</generator><language>ja</language><atom:link href="https://motonarikambara.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Education</title><link>https://motonarikambara.github.io/profile/education/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://motonarikambara.github.io/profile/education/</guid><description> Apr. 2023 – Sep. 2025 Keio University Ph.D. student (Information and Computer Science) Apr. 2021 – Mar. 2023 Keio University Master (Information and Computer Science) Apr. 2017 – Mar. 2021 Keio University Bachelor (Information and Computer Science)</description></item><item><title>Employment</title><link>https://motonarikambara.github.io/profile/employment/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://motonarikambara.github.io/profile/employment/</guid><description> Sep. 2025 - Visiting Researcher at SMI Lab, Keio University, Kanagawa, Japan Sep. 2025 - JSPS Research Fellowship for Young Scientist PD Jun. 2023 - Dec. 2023 Internship at Mitsubishi Electric Research Laboratories (MERL), Boston, U.S. Apr. 2023 - Sep. 2025 JSPS Research Fellowship for Young Scientist DC1</description></item><item><title>Journal</title><link>https://motonarikambara.github.io/publications/journal/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://motonarikambara.github.io/publications/journal/</guid><description>M. Kambara and K. Suigura, &amp;ldquo;Pre-Manipulation Alignment Prediction with Parallel Deep State-Space and Transformer Models&amp;rdquo;, Advanced Robotics, Vol. 39, Issue 13, pp. 806–816, 2025. DOI: 10.1080/01691864.2025.2532610. PDF K. Katsumata, M. Kambara, D. Yashima, R. Korekata, and K. Sugiura, “Mobile Manipulation Instruction Generation from Multiple Images with Automatic Metric Enhancement”, IEEE Robotics and Automation Letters, Vol. 10, Issue 3, pp. 3022-3029, 2025. DOI: 10.1109/LRA.2025.3539086. PDF T. Komatsu, M. Kambara, S. Hatanaka, H.</description></item><item><title/><link>https://motonarikambara.github.io/home/recentworks/crt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://motonarikambara.github.io/home/recentworks/crt/</guid><description>Case Relation Transformer: A Crossmodal Language Generation Model for Fetching Instructions There have been many studies in robotics to improve the communication skills of domestic service robots. Most studies, however, have not fully benefited from recent advances in deep neural networks because the training datasets are not large enough. In this paper, our aim is crossmodal language generation.
We propose the Case Relation Transformer (CRT), which generates a fetching instruction sentence from an image, such as “Move the blue flip-flop to the lower left box.</description></item><item><title/><link>https://motonarikambara.github.io/home/recentworks/lambdarepformer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://motonarikambara.github.io/home/recentworks/lambdarepformer/</guid><description>Relational Future Captioning Model for Explaining Likely Collisions in Daily Tasks Domestic service robots that support daily tasks are a promising solution for elderly or disabled people. It is crucial for domestic service robots to explain the collision risk before they perform actions.
In this paper, our aim is to generate a caption about a future event. We propose the Relational Future Captioning Model (RFCM), a crossmodal language generation model for the future captioning task.</description></item><item><title/><link>https://motonarikambara.github.io/home/recentworks/rfcm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://motonarikambara.github.io/home/recentworks/rfcm/</guid><description>Relational Future Captioning Model for Explaining Likely Collisions in Daily Tasks Domestic service robots that support daily tasks are a promising solution for elderly or disabled people. It is crucial for domestic service robots to explain the collision risk before they perform actions.
In this paper, our aim is to generate a caption about a future event. We propose the Relational Future Captioning Model (RFCM), a crossmodal language generation model for the future captioning task.</description></item><item><title>International Conference</title><link>https://motonarikambara.github.io/publications/inter_conf/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://motonarikambara.github.io/publications/inter_conf/</guid><description>C. Hori, M. Kambara, K. Sugiura, K. Ota, S. Khurana, S. Jain, R. Corcodel, D. Jha, D. Romeres, J. Le Roux. “Interactive Robot Action Replanning using Multimodal LLM Trained from Human Demonstration Videos”, IEEE ICASSP, pp.6390–6394, 2025. PDF M. Goko*, M. Kambara*, S. Otsuki, D. Saito, and K. Sugiura (*: Equal contribution), “Task Success Prediction for Open-Vocabulary Manipulation Based on Multi-Level Aligned Representations”, CoRL, 2024. project page, PDF K. Kaneda, S.</description></item><item><title>Award</title><link>https://motonarikambara.github.io/publications/award/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://motonarikambara.github.io/publications/award/</guid><description> K. Kaneda, R. Korekata, Y. Wada, S. Nagashima, M. Kambara, Y. Iioka, H. Matsuo, Y. Imai, T. Nishimura, K. Sugiura, CVPR 2023 Embodied AI Workshop DialFRED Challenge 1st Prize, 6/19/2023. M. Kambara, Y. Yoshida, K. Kaneda, S. Otsuki, R. Korekata, H. Matsuo, Y. Wada, W. Yang, K. Sugiura, Honorable Mention Award, REVERIE Challenge @ CSIG 2022, 8/19/2022.</description></item><item><title>Skills</title><link>https://motonarikambara.github.io/profile/skill/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://motonarikambara.github.io/profile/skill/</guid><description> Python, Pytorch, Tensorflow2, JAX ROS 1&amp;amp;2 GitHub: @motonarikambara</description></item><item><title>Domestic Conference</title><link>https://motonarikambara.github.io/publications/domestic_conf/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://motonarikambara.github.io/publications/domestic_conf/</guid><description>勝又圭, 神原元就, 八島大地, 是方諒介, 杉浦孔明: “物体操作指示文生成モデルに基づくモバイルマニピュレーションのためのデータセット拡張”, 2025年度 人工知能学会全国大会, 1Win4-49, 2025. 後神美結, 神原元就, 齋藤大地, 小槻誠太郎, 杉浦孔明: “多階層アラインメント視覚表現に基づくリアルタイム物体操作タスク成功判定”, 2025年度 人工知能学会全国大会, 1Win4-51, 2025. 神原元就, 杉浦孔明: “深層状態空間モデルおよびtransformerの並列機構による物体操作前アラインメント予測”, 2025年度 人工知能学会全国大会, 1Win4-55, 2025. 神原元就, 杉浦孔明: “オフライン軌道生成による軌道に基づくopen-vocabulary物体操作タスクにおける将来成否予測”, 第42回日本ロボット学会学術講演会, 3D1-06, 2024. 勝又圭, 神原元就, 杉浦孔明: “自動評価尺度を用いた強化学習およびマルチモーダル基盤モデルに基づく物体操作指示文生成”, 第42回日本ロボット学会学術講演会, 3D3-03, 2024. 後神美結, 神原元就, 小槻誠太郎, 杉浦孔明: “マルチモーダルLLM及び視覚言語基盤モデルに基づく多階層アラインメント表現による物体操作タスク成功判定”, 第42回日本ロボット学会学術講演会, 1D2-07, 2024. 松尾榛夏, 神原元就, 杉浦孔明: “マルチモーダル基盤モデルと劣モジュラ最適化に基づく移動ロボットの環境探索”, 2024年度 人工知能学会全国大会, 4O3-OS-16e-03, 2024. 齋藤大地, 神原元就, 九曜克之, 杉浦孔明: “マルチモーダルLLMおよび視覚言語基盤モデルに基づく大規模物体操作データセットにおけるタスク成功判定”, 2024年度 人工知能学会全国大会, 3O1-OS-16b-02, 2024. 西村喬行, 九曜克之, 神原元就, 杉浦孔明: “マルチモーダル基盤モデルと最適輸送を用いたポリゴンマッチングによる参照表現セグメンテーション”, 2024年度 人工知能学会全国大会, 2O6-OS-16a-02, 2024.</description></item><item><title/><link>https://motonarikambara.github.io/home/subheader/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://motonarikambara.github.io/home/subheader/</guid><description>I am Motonari Kambara. I am a Research Fellow in SMI Lab at Keio University.
My academic and research interests are deeply rooted in the intersection of Embodied AI, Vision and Language (V&amp;amp;L), as well as the development and application of domestic service robots.</description></item><item><title>Application development</title><link>https://motonarikambara.github.io/home/research/service3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://motonarikambara.github.io/home/research/service3/</guid><description>&lt;p>I don&amp;rsquo;t think they tried to market it to the billionaire, spelunking, base-jumping crowd. i did the same thing to gandhi, he didn&amp;rsquo;t eat for three weeks. i once heard a wise man say there are no perfect men.&lt;/p></description></item><item><title>Custom website design</title><link>https://motonarikambara.github.io/home/research/service1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://motonarikambara.github.io/home/research/service1/</guid><description>&lt;p>I don&amp;rsquo;t think they tried to market it to the billionaire, spelunking, base-jumping crowd. i did the same thing to gandhi, he didn&amp;rsquo;t eat for three weeks. i once heard a wise man say there are no perfect men.&lt;/p></description></item><item><title>SEO &amp; SEM services</title><link>https://motonarikambara.github.io/home/research/service4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://motonarikambara.github.io/home/research/service4/</guid><description>&lt;p>I don&amp;rsquo;t think they tried to market it to the billionaire, spelunking, base-jumping crowd. i did the same thing to gandhi, he didn&amp;rsquo;t eat for three weeks. i once heard a wise man say there are no perfect men.&lt;/p></description></item><item><title>Wordpress integration</title><link>https://motonarikambara.github.io/home/research/service2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://motonarikambara.github.io/home/research/service2/</guid><description>&lt;p>I don&amp;rsquo;t think they tried to market it to the billionaire, spelunking, base-jumping crowd. i did the same thing to gandhi, he didn&amp;rsquo;t eat for three weeks. i once heard a wise man say there are no perfect men.&lt;/p></description></item></channel></rss>